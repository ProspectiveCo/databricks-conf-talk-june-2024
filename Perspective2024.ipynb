{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aabe215e-cf89-48fb-b227-a384be00f559",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import VBox, HBox\n",
    "from pyspark.sql import SparkSession\n",
    "from perspective import PerspectiveWidget\n",
    "\n",
    "from helpers.data import (\n",
    "    HOST,\n",
    "    MACHINES_PORT,\n",
    "    USAGE_PORT,\n",
    "    STATUS_PORT,\n",
    "    JOBS_PORT,\n",
    ")\n",
    "from helpers.spark import (\n",
    "    MACHINE_SCHEMA,\n",
    "    MACHINE_SCHEMA_SPARK,\n",
    "    USAGE_SCHEMA,\n",
    "    USAGE_SCHEMA_SPARK,\n",
    "    STATUS_SCHEMA,\n",
    "    STATUS_SCHEMA_SPARK,\n",
    "    JOBS_SCHEMA,\n",
    "    JOBS_SCHEMA_SPARK,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b4aa82d-9e60-4419-8fc7-47c2697c0ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important imports\n",
    "from helpers.spark import (\n",
    "    get_df_from_server,\n",
    "    push_to_perspective,\n",
    ")\n",
    "from helpers.fastapi import (\n",
    "    perspective_spark_bridge,\n",
    "    start_server,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "183c0a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.data import machines, usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d43b993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'machine_id': '4332a1fd80ab',\n",
       " 'kind': 'edge',\n",
       " 'cores': 4,\n",
       " 'region': 'eu',\n",
       " 'zone': 'B'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = machines()\n",
    "m[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "387dd68a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'machine_id': '4332a1fd80ab',\n",
       " 'kind': 'edge',\n",
       " 'cores': 4,\n",
       " 'region': 'eu',\n",
       " 'zone': 'B',\n",
       " 'cpu': 0,\n",
       " 'mem': 0,\n",
       " 'free': 100,\n",
       " 'network': 0,\n",
       " 'disk': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = usage(m[0])\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3a3ac11-da91-4686-a853-1ae0cc6ab39b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'machine_id': '4332a1fd80ab',\n",
       " 'kind': 'edge',\n",
       " 'cores': 4,\n",
       " 'region': 'eu',\n",
       " 'zone': 'B',\n",
       " 'cpu': 38.75,\n",
       " 'mem': 59.79,\n",
       " 'free': 40.21,\n",
       " 'network': 74.57,\n",
       " 'disk': 69.98}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = usage(u)\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f808e2d3-29bc-413f-b6e4-5b33c7322904",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/11 21:58:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Perspective Demo\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2938e11-ad08-455a-aad7-4398e3b6f338",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/11 21:58:29 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n",
      "24/06/11 21:58:29 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n",
      "24/06/11 21:58:29 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n",
      "24/06/11 21:58:29 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n"
     ]
    }
   ],
   "source": [
    "# Get spark streaming dfs\n",
    "machines_df = get_df_from_server(spark, MACHINE_SCHEMA_SPARK, HOST, MACHINES_PORT)\n",
    "usage_df = get_df_from_server(spark, USAGE_SCHEMA_SPARK, HOST, USAGE_PORT)\n",
    "status_df = get_df_from_server(spark, STATUS_SCHEMA_SPARK, HOST, STATUS_PORT)\n",
    "jobs_df = get_df_from_server(spark, JOBS_SCHEMA_SPARK, HOST, JOBS_PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d64d516-7285-46ec-9c72-31e8985d5956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct 4 separate perspective widgets. Each will have its own table internally\n",
    "machines_widget = PerspectiveWidget(MACHINE_SCHEMA, index=\"machine_id\", settings=False)\n",
    "usage_widget = PerspectiveWidget(USAGE_SCHEMA, index=\"machine_id\", settings=False)\n",
    "status_widget = PerspectiveWidget(STATUS_SCHEMA, index=\"machine_id\", sort=[[\"last_update\", \"desc\"]], settings=False)\n",
    "jobs_widget = PerspectiveWidget(JOBS_SCHEMA, sort=[[\"start_time\", \"desc\"]], settings=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ad0165a-a8cb-4cbd-895f-631e21392e09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "823ccc11d1234b108743536052b40fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(PerspectiveWidget(columns=['machine_id', 'kind', 'cores', 'region', 'zone'], setâ€¦"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a little bit of layout with ipywidgets\n",
    "VBox(children=[\n",
    "    HBox(children=[machines_widget, usage_widget]),\n",
    "    HBox(children=[status_widget, jobs_widget]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f949043b-00e1-4fd8-956c-3047f09ff294",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:Listening on http://localhost:51701\n",
      "INFO:     Started server process [22]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:51701 (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "app = perspective_spark_bridge(\n",
    "    {\n",
    "        \"machines\": machines_widget,\n",
    "        \"usage\": usage_widget,\n",
    "        \"status\": status_widget,\n",
    "        \"jobs\": jobs_widget,\n",
    "    }\n",
    ")\n",
    "port = start_server(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5d55e4ab-3737-4f71-b590-a95e9686481d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/10 23:15:09 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-57efb414-d05a-4fb1-baaf-629d5b6ccb6f. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/06/10 23:15:09 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/06/10 23:15:10 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-d8beda44-1cbc-4ffe-95f8-b46b4bb8e235. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/06/10 23:15:10 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/06/10 23:15:10 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-32104b55-8d73-476e-b86a-34fe54932d73. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/06/10 23:15:10 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/06/10 23:15:10 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-a47ff0c0-fb28-475d-b64a-1bd4f8e33b9a. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/06/10 23:15:10 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/06/10 23:15:10 WARN TextSocketMicroBatchStream: Stream closed by data:8081\n"
     ]
    }
   ],
   "source": [
    "push_to_perspective(machines_df, \"machines\", \"localhost\", port)\n",
    "push_to_perspective(usage_df, \"usage\", \"localhost\", port)\n",
    "push_to_perspective(status_df, \"status\", \"localhost\", port)\n",
    "push_to_perspective(jobs_df, \"jobs\", \"localhost\", port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e859237-4be3-4994-83b1-78e3eaea19d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "status_widget.plugin = \"X Bar\"\n",
    "status_widget.group_by = [\"status\"]\n",
    "status_widget.columns = [\"machine_id\"]\n",
    "status_widget.aggregates = {\"status\": \"last\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5f1855c",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_widget.group_by = [\"machine_id\"]\n",
    "jobs_widget.columns = [\"job_id\", \"name\", \"units\", \"start_time\", \"end_time\"],\n",
    "jobs_widget.aggregates = {\"job_id\": \"count\", \"name\": \"last\", \"units\": \"sum\", \"start_time\": \"last\", \"end_time\": \"last\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c27f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
